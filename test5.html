<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>3D Glasses Overlay in 2D Canvas</title>
  <style>
    body { margin: 0; overflow: hidden; }
    canvas { position: absolute; top: 0; left: 0; }
  </style>
</head>
<body>
  <video id="video" width="640" height="480" autoplay muted playsinline></video>
  <canvas id="canvas"></canvas>
  
  <!-- Load Three.js and GLTFLoader -->
  <script src="https://cdn.jsdelivr.net/npm/three@0.132.2/build/three.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/three@0.132.2/examples/js/loaders/GLTFLoader.js"></script>
  <script src="face-api.js"></script>
<script>
    const video = document.getElementById('video');
    const canvas = document.getElementById('canvas');
    const ctx = canvas.getContext('2d', { willReadFrequently: true });

           const isMobile = window.innerWidth < window.innerHeight;

            var width, height;
            if (isMobile) {
                width = 480;
                height = 640;
            } else {
                width = 640;
                height = 480;
            }

  // Set canvas size to video size
    canvas.width = video.width = width;
    canvas.height = video.height = height;

    // Initialize Three.js scene, camera, and renderer
    const scene = new THREE.Scene();
    const camera = new THREE.PerspectiveCamera(75, canvas.width / canvas.height, 0.1, 1000);
    const renderer = new THREE.WebGLRenderer({ canvas: document.createElement('canvas') });
    renderer.setSize(640, 480); // Match the video size
    const renderTarget = new THREE.WebGLRenderTarget(width, height); // Render to this target

    // Load the glasses 3D model (glasses.glb)
    const loader = new THREE.GLTFLoader();
    let glasses = null;
        let glasses1 = new Image();
        glasses1.src = 'glasses.png';

    loader.load('glasses.glb', (gltf) => {
      glasses = gltf.scene;
      scene.add(glasses);
      
      // Adjust the model position and scale based on the eye positions
      glasses.scale.set(0.5, 0.5, 0.5);  // Example scale for the glasses model
      glasses.position.set(0, 0, -0.1);   // Adjust the glasses' position in front of the camera
    });

    // Set up video stream (webcam)
    navigator.mediaDevices.getUserMedia({ video: true })
      .then((stream) => {
        video.srcObject = stream;
       // video.play();

       // video.onloadedmetadata = () => {
          // Start rendering loop after video is ready
       //   animate();
       // };
      })
      .catch((err) => {
        console.error('Error accessing webcam:', err);
      });

          // 加載 face-api.js 模型
        async function loadFaceApi() {
            try {
            console.log('loadFaceApi ssdMobilenetv1 start');
            await faceapi.nets.ssdMobilenetv1.load('/');
            console.log('loadFaceApi faceLandmark68Net start');
            await faceapi.nets.faceLandmark68Net.load('/');
            console.log('loadFaceApi faceRecognitionNet start');
            await faceapi.nets.faceRecognitionNet.load('/');
            console.log('loadFaceApi end');
            detectFace();
            } catch (error) {
             console.error('加載模型時發生錯誤:', error);
                }
        }


    // Eye position and scale (example values, these should come from face detection)
    let eyeX = 200;  // Example x position of the eyes
    let eyeY = 150;  // Example y position of the eyes
    let eyeWidth = 220; // Width of the glasses
    let eyeHeight = 80; // Height of the glasses

    // Function to render the video and overlay 3D glasses as 2D
     async function detectFace() {
            const detections = await faceapi.detectAllFaces(video).withFaceLandmarks();
            if (detections.length > 0) {
                const landmarks = detections[0].landmarks;
                const leftEye = landmarks.getLeftEye();
                const rightEye = landmarks.getRightEye();
              eyeX = (leftEye[0].x + rightEye[3].x) / 2;
              eyeY = (leftEye[3].y + rightEye[3].y) / 2;
              eyeWidth = (eyeX - leftEye[0].x) * 2 * 1.6;
              eyeHeight = (eyeX - leftEye[0].x) * 2 * 1.6 / 220 * 80 ;

             // Draw the current video frame on the canvas
      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

      //ctx.fillStyle = 'blue';
      //ctx.fillRect(eyeX - eyeWidth / 2, eyeY - eyeHeight / 2, eyeWidth, eyeHeight);  // Draw blue background where the glasses will go

      // If the glasses model is loaded, render it as a 2D texture
      if (glasses) {
        // Render the glasses model to the render target (off-screen)
        renderer.setRenderTarget(renderTarget);
        renderer.render(scene, camera);

        // Reset render target to the default framebuffer (for normal canvas rendering)
        renderer.setRenderTarget(null);

        // Read pixels from the render target
        const pixels = new Uint8Array(4 * renderTarget.width * renderTarget.height);
        renderer.readRenderTargetPixels(renderTarget, 0, 0, renderTarget.width, renderTarget.height, pixels);

        // Create a new image to hold the pixel data
        const imageData = ctx.createImageData(renderTarget.width, renderTarget.height);
        for (let i = 0; i < pixels.length; i += 4) {
          imageData.data[i] = pixels[i];     // Red
          imageData.data[i + 1] = pixels[i+1]; // Green
          imageData.data[i + 2] = pixels[i+2]; // Blue
          imageData.data[i + 3] = pixels[i+3]; // Alpha
        }

        // Put the image data on the canvas
        //console.log('模型1:',eyeX - eyeWidth / 2);
        //console.log('模型2-0:',leftEye[0].x + '||' + leftEye[0].y);
        //console.log('模型2-1:',leftEye[1].x + '||' + leftEye[1].y);
        //console.log('模型2-2:',leftEye[2].x + '||' + leftEye[2].y);
        //console.log('模型2-3:',leftEye[3].x + '||' + leftEye[3].y);
        //console.log('模型2-4:',leftEye[4].x + '||' + leftEye[4].y);
        //console.log('模型2-5:',leftEye[5].x + '||' + leftEye[5].y);
        //console.log('模型3-0:',rightEye[0].x + '||' + rightEye[0].y);
        //console.log('模型3-1:',rightEye[1].x + '||' + rightEye[1].y);
        //console.log('模型3-2:',rightEye[2].x + '||' + rightEye[2].y);
        //console.log('模型3-3:',rightEye[3].x + '||' + rightEye[3].y);
        //console.log('模型3-4:',rightEye[4].x + '||' + rightEye[4].y);
        //console.log('模型3-5:',rightEye[5].x + '||' + rightEye[5].y);
        //ctx.putImageData(imageData, eyeX - eyeWidth / 2, eyeY - eyeHeight / 2);
        ctx.drawImage(glasses1, eyeX - eyeWidth / 2, eyeY - eyeHeight / 2, eyeWidth, eyeHeight);
        

        // Optional: Draw a bounding box to show glasses' position
        ctx.strokeStyle = 'red';
        ctx.lineWidth = 2;
        //console.log('Rect1:',eyeX - eyeWidth / 2);
        //console.log('Rect2:',eyeY - eyeHeight / 2);
        ctx.strokeRect(eyeX - eyeWidth / 2, eyeY - eyeHeight / 2, eyeWidth, eyeHeight);
      }
              }
requestAnimationFrame(detectFace);
     }
  loadFaceApi();
    function animate() {
      requestAnimationFrame(animate);
      renderer.render(scene, camera);

    }
  animate();
  
  </script>
</body>
</html>
