<!DOCTYPE html>
<html lang="zh">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AR è©¦æˆ´çœ¼é¡</title>
    <style>
        body {
            text-align: center;
            background: black;
        }
        video, canvas {
            position: absolute;
            left: 50%;
            top: 50%;
            transform: translate(-50%, -50%);
        }
    </style>
</head>
<body>
    <video id="video" autoplay playsinline></video>
    <canvas id="canvas"></canvas>

    <!-- è¼‰å…¥ OpenCV.js -->
    <script src="https://docs.opencv.org/4.x/opencv.js"></script>

    <script>
        cv['onRuntimeInitialized'] = () => {
            console.log("âœ… OpenCV.js è¼‰å…¥å®Œæˆï¼");
            startFaceDetection();
        };

        let video = document.getElementById('video');
        let canvas = document.getElementById('canvas');
        let ctx = canvas.getContext('2d');
        let glasses = new Image();
        glasses.src = 'https://stevenhsu75.github.io/glasses.png';  
        let faceCascade = null;
        let cap = null;

        function startFaceDetection() {
            if (typeof cv === 'undefined') {
                console.error("âŒ OpenCV.js å°šæœªè¼‰å…¥ï¼");
                return;
            }

            faceCascade = new cv.CascadeClassifier();
            faceCascade.load('https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml');

            navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
                video.srcObject = stream;

                // ç¢ºä¿ video å½±åƒå¯ç”¨
                video.onloadeddata = () => {
                    console.log("ğŸ¥ ç›¸æ©Ÿå·²å°±ç·’ï¼");
                    video.play();
                    canvas.width = video.videoWidth;
                    canvas.height = video.videoHeight;

                    // ä½¿ç”¨ OpenCV è®€å–å½±åƒ
                    cap = new cv.VideoCapture(video);

                    // æ¯ 100ms åŸ·è¡Œ detectFace
                    setInterval(detectFace, 100);
                };
            }).catch((error) => {
                console.error("âŒ ç„¡æ³•å•Ÿå‹•ç›¸æ©Ÿ:", error);
            });
        }

        function detectFace() {
            if (!cap) {
                console.warn("ğŸš¨ VideoCapture å°šæœªåˆå§‹åŒ–");
                return;
            }

            if (video.videoWidth === 0 || video.videoHeight === 0) {
                console.warn("ğŸš¨ è¦–è¨Šæœªæº–å‚™å¥½ï¼Œè·³éåµæ¸¬");
                return;
            }

            let frame = new cv.Mat(video.videoHeight, video.videoWidth, cv.CV_8UC4);
            cap.read(frame);

            let gray = new cv.Mat();
            cv.cvtColor(frame, gray, cv.COLOR_RGBA2GRAY);

            let faces = new cv.RectVector();
            let msize = new cv.Size(100, 100);
            faceCascade.detectMultiScale(gray, faces, 1.1, 3, 0, msize, msize);

            ctx.clearRect(0, 0, canvas.width, canvas.height);
            ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

            if (faces.size() > 0) {
                let face = faces.get(0);
                let eyeX = face.x + face.width * 0.15;
                let eyeY = face.y + face.height * 0.3;
                let eyeWidth = face.width * 0.7;
                let eyeHeight = eyeWidth / 3;

                console.log("ğŸ‘“ åµæ¸¬åˆ°è‡‰éƒ¨ï¼Œç¹ªè£½çœ¼é¡ï¼", eyeX, eyeY, eyeWidth, eyeHeight);
                ctx.drawImage(glasses, eyeX, eyeY, eyeWidth, eyeHeight);
            } else {
                console.log("âŒ æœªåµæ¸¬åˆ°è‡‰éƒ¨ï¼");
            }

            frame.delete();
            gray.delete();
            faces.delete();
        }
    </script>
</body>
</html>
