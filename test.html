<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Face and Eye Detection</title>
    <script defer src="face-api.js"></script>
    <style>
        body {
            text-align: center;
        }
        video {
            position: absolute;
            margin: 0;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        canvas {
            position: absolute;
            margin: 0;
            top: 0;
            left: 0;
            z-index: 1;
        }
    </style>
</head>
<body>
    <h1>Real-time Face and Eye Detection</h1>
    <video id="video" autoplay muted></video>
    <script>
        // 載入 face-api.js 模型
        async function setup() {
            // 載入模型
            await faceapi.nets.tinyFaceDetector.load('/');
            await faceapi.loadFaceLandmarkModel('/')
            //await faceapi.nets.ssdMobilenetv1.load('/'); // 指定模型目錄
            //await faceapi.nets.faceLandmark68Net.load('/');
            //await faceapi.nets.faceRecognitionNet.load('/');

            // 獲取視頻元素
            const video = document.getElementById('video');

            // 使用 webcam 捕獲視頻
            navigator.mediaDevices.getUserMedia({ video: {} })
                .then((stream) => {
                    video.srcObject = stream;
                })
                .catch((err) => {
                    console.error('Error accessing webcam: ', err);
                });

            // 當視頻開始播放時啟動檢測
            video.onplay = async () => {
                const canvas = faceapi.createCanvasFromMedia(video);
                document.body.append(canvas);
                const displaySize = { width: video.width, height: video.height };
                faceapi.matchDimensions(canvas, displaySize);

                // 每幾幀檢測一次人臉
                setInterval(async () => {
                    const detections = await faceapi.detectAllFaces(video)
                        .withFaceLandmarks()  // 獲取面部標誌點
                        .withFaceDescriptors();

                    // 將檢測結果畫到畫布上
                    const resizedDetections = faceapi.resizeResults(detections, displaySize);
                    canvas.clear();
                    faceapi.draw.drawDetections(canvas, resizedDetections);
                    faceapi.draw.drawFaceLandmarks(canvas, resizedDetections);

                    // 獲取眼睛位置並畫出
                    resizedDetections.forEach(detection => {
                        const landmarks = detection.landmarks;
                        const leftEye = landmarks.getLeftEye();
                        const rightEye = landmarks.getRightEye();

                        // 畫出左眼和右眼的位置
                        leftEye.forEach(point => {
                            const x = point.x;
                            const y = point.y;
                            const radius = 2;
                            canvas.getContext('2d').beginPath();
                            canvas.getContext('2d').arc(x, y, radius, 0, Math.PI * 2);
                            canvas.getContext('2d').fillStyle = 'green';
                            canvas.getContext('2d').fill();
                        });

                        rightEye.forEach(point => {
                            const x = point.x;
                            const y = point.y;
                            const radius = 2;
                            canvas.getContext('2d').beginPath();
                            canvas.getContext('2d').arc(x, y, radius, 0, Math.PI * 2);
                            canvas.getContext('2d').fillStyle = 'green';
                            canvas.getContext('2d').fill();
                        });
                    });
                }, 100);
            };
        }

        setup();
    </script>
</body>
</html>
